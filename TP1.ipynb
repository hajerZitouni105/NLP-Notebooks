{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ZjPSmdT1k2SJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords , wordnet ,state_union\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import Text\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "pd.pandas.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspellchecker"
      ],
      "metadata": {
        "id": "vRHDUkzWnuPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sobMg3f_k2SN"
      },
      "source": [
        "## Partie I :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJbkNnJ4k2SQ"
      },
      "source": [
        "### Exercice 1 :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hmYcnYUk2SR"
      },
      "source": [
        "##### Question 1 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8wbJgKXk2SS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"spam_ham_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7qUYCVuk2SS",
        "outputId": "a02e7095-7888-41ac-9da0-dfc23013fddc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 label                                               text  \\\n",
              "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "\n",
              "   label_num  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          1  \n",
              "4          0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE_Hz8qFk2SV",
        "outputId": "9209172f-50cb-451b-989d-04e28f4a1b78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "label         0\n",
              "text          0\n",
              "label_num     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA5hUA_yk2SW"
      },
      "source": [
        "##### Question 2 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ZI1NDuk2SX",
        "outputId": "f1024bc9-f754-4a88-c8da-11114667fb29"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  Subject: enron methanol ; meter # : 988291\\r\\n...   ham\n",
              "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   ham\n",
              "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   ham\n",
              "3  Subject: photoshop , windows , office . cheap ...  spam\n",
              "4  Subject: re : indian springs\\r\\nthis deal is t...   ham"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[:, ['text', 'label']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNmG0UmMk2SY"
      },
      "source": [
        "##### Question 3 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeC6JMMwk2SY",
        "outputId": "4c56acdc-db0c-4f98-9e80-d6fd25aeb17e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5171, 4)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-v6edQ5k2SZ",
        "outputId": "3fc49617-e2fa-496d-e33c-509524db4c3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "ham     3672\n",
              "spam    1499\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQJ7cQIk2SZ"
      },
      "source": [
        "##### Question 4 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhEDIxpak2Sa"
      },
      "outputs": [],
      "source": [
        "def delete_ponctuation(text):\n",
        "    text2 = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return text2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8y0zzvtk2Sa",
        "outputId": "9731049a-b1e9-4dd8-a2e3-94bb4e28e353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "la ponctuation est  enlevéede cette  phrase\n"
          ]
        }
      ],
      "source": [
        "text = \"->la ponctuation, est : enlevée_de cette ? phrase.!\"\n",
        "print(delete_ponctuation(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49or_R7ik2Sb"
      },
      "source": [
        "##### Question 5 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4ud6e-rk2Sb"
      },
      "outputs": [],
      "source": [
        "df['text_p'] = df['text'].apply(delete_ponctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-z1tTS6k2Sb",
        "outputId": "72cea307-b49d-493a-eb63-b42f65fb7365"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>text_p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Subject photoshop  windows  office  cheap  mai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 label                                               text  \\\n",
              "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "\n",
              "   label_num                                             text_p  \n",
              "0          0  Subject enron methanol  meter   988291\\r\\nthis...  \n",
              "1          0  Subject hpl nom for january 9  2001\\r\\n see at...  \n",
              "2          0  Subject neon retreat\\r\\nho ho ho  we  re aroun...  \n",
              "3          1  Subject photoshop  windows  office  cheap  mai...  \n",
              "4          0  Subject re  indian springs\\r\\nthis deal is to ...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym9j3_Ekk2Sc"
      },
      "source": [
        "##### Question 6 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwu6avS6k2Sc"
      },
      "outputs": [],
      "source": [
        "df['text_lower'] = df['text_p'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw6qDBDlk2Sc",
        "outputId": "1ef0554c-1777-42e1-f89c-28aa3625fe50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>text_p</th>\n",
              "      <th>text_lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "      <td>subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "      <td>subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "      <td>subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Subject photoshop  windows  office  cheap  mai...</td>\n",
              "      <td>subject photoshop  windows  office  cheap  mai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "      <td>subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 label                                               text  \\\n",
              "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "\n",
              "   label_num                                             text_p  \\\n",
              "0          0  Subject enron methanol  meter   988291\\r\\nthis...   \n",
              "1          0  Subject hpl nom for january 9  2001\\r\\n see at...   \n",
              "2          0  Subject neon retreat\\r\\nho ho ho  we  re aroun...   \n",
              "3          1  Subject photoshop  windows  office  cheap  mai...   \n",
              "4          0  Subject re  indian springs\\r\\nthis deal is to ...   \n",
              "\n",
              "                                          text_lower  \n",
              "0  subject enron methanol  meter   988291\\r\\nthis...  \n",
              "1  subject hpl nom for january 9  2001\\r\\n see at...  \n",
              "2  subject neon retreat\\r\\nho ho ho  we  re aroun...  \n",
              "3  subject photoshop  windows  office  cheap  mai...  \n",
              "4  subject re  indian springs\\r\\nthis deal is to ...  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rutWsyXZk2Sd"
      },
      "source": [
        "##### Question 7 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56vYxR3Lk2Sd"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yM9y4I2k2Sd"
      },
      "outputs": [],
      "source": [
        "df['text_tokenise'] = df['text_lower'].apply(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzaTnTfzk2Sd",
        "outputId": "affb16dc-7ff1-49ed-ead8-410ec68167e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>text_p</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_tokenise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "      <td>subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "      <td>[subject, enron, methanol, meter, 988291, this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "      <td>subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "      <td>[subject, hpl, nom, for, january, 9, 2001, see...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "      <td>subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, we, re, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Subject photoshop  windows  office  cheap  mai...</td>\n",
              "      <td>subject photoshop  windows  office  cheap  mai...</td>\n",
              "      <td>[subject, photoshop, windows, office, cheap, m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "      <td>subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "      <td>[subject, re, indian, springs, this, deal, is,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 label                                               text  \\\n",
              "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "\n",
              "   label_num                                             text_p  \\\n",
              "0          0  Subject enron methanol  meter   988291\\r\\nthis...   \n",
              "1          0  Subject hpl nom for january 9  2001\\r\\n see at...   \n",
              "2          0  Subject neon retreat\\r\\nho ho ho  we  re aroun...   \n",
              "3          1  Subject photoshop  windows  office  cheap  mai...   \n",
              "4          0  Subject re  indian springs\\r\\nthis deal is to ...   \n",
              "\n",
              "                                          text_lower  \\\n",
              "0  subject enron methanol  meter   988291\\r\\nthis...   \n",
              "1  subject hpl nom for january 9  2001\\r\\n see at...   \n",
              "2  subject neon retreat\\r\\nho ho ho  we  re aroun...   \n",
              "3  subject photoshop  windows  office  cheap  mai...   \n",
              "4  subject re  indian springs\\r\\nthis deal is to ...   \n",
              "\n",
              "                                       text_tokenise  \n",
              "0  [subject, enron, methanol, meter, 988291, this...  \n",
              "1  [subject, hpl, nom, for, january, 9, 2001, see...  \n",
              "2  [subject, neon, retreat, ho, ho, ho, we, re, a...  \n",
              "3  [subject, photoshop, windows, office, cheap, m...  \n",
              "4  [subject, re, indian, springs, this, deal, is,...  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPlsj2sqk2Sd"
      },
      "source": [
        "##### Question 8 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4_Bi8SQk2Se",
        "outputId": "0ff21651-3b5e-4a16-f965-bb95c6ecd65d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words_english = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGzR3E2Gk2Se",
        "outputId": "65e27b06-aa3e-4906-fd83-36eac91496e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words_english[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwujevEDk2Se"
      },
      "source": [
        "##### Question 9 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYql9tgAk2Sg"
      },
      "outputs": [],
      "source": [
        "stop_words_english = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIGRlT7Kk2Sg"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens):\n",
        "    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words_english]\n",
        "    return tokens_no_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNKOXHfgk2Sh"
      },
      "outputs": [],
      "source": [
        "df['no_stopwords'] = df['text_tokenise'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiZNW68zk2Sh",
        "outputId": "42ed5a8d-75ed-4783-f94e-435a4e3a368b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>text_p</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_tokenise</th>\n",
              "      <th>no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "      <td>subject enron methanol  meter   988291\\r\\nthis...</td>\n",
              "      <td>[subject, enron, methanol, meter, 988291, this...</td>\n",
              "      <td>[subject, enron, methanol, meter, 988291, foll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "      <td>subject hpl nom for january 9  2001\\r\\n see at...</td>\n",
              "      <td>[subject, hpl, nom, for, january, 9, 2001, see...</td>\n",
              "      <td>[subject, hpl, nom, january, 9, 2001, see, att...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "      <td>subject neon retreat\\r\\nho ho ho  we  re aroun...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, we, re, a...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Subject photoshop  windows  office  cheap  mai...</td>\n",
              "      <td>subject photoshop  windows  office  cheap  mai...</td>\n",
              "      <td>[subject, photoshop, windows, office, cheap, m...</td>\n",
              "      <td>[subject, photoshop, windows, office, cheap, m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>Subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "      <td>subject re  indian springs\\r\\nthis deal is to ...</td>\n",
              "      <td>[subject, re, indian, springs, this, deal, is,...</td>\n",
              "      <td>[subject, indian, springs, deal, book, teco, p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 label                                               text  \\\n",
              "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "\n",
              "   label_num                                             text_p  \\\n",
              "0          0  Subject enron methanol  meter   988291\\r\\nthis...   \n",
              "1          0  Subject hpl nom for january 9  2001\\r\\n see at...   \n",
              "2          0  Subject neon retreat\\r\\nho ho ho  we  re aroun...   \n",
              "3          1  Subject photoshop  windows  office  cheap  mai...   \n",
              "4          0  Subject re  indian springs\\r\\nthis deal is to ...   \n",
              "\n",
              "                                          text_lower  \\\n",
              "0  subject enron methanol  meter   988291\\r\\nthis...   \n",
              "1  subject hpl nom for january 9  2001\\r\\n see at...   \n",
              "2  subject neon retreat\\r\\nho ho ho  we  re aroun...   \n",
              "3  subject photoshop  windows  office  cheap  mai...   \n",
              "4  subject re  indian springs\\r\\nthis deal is to ...   \n",
              "\n",
              "                                       text_tokenise  \\\n",
              "0  [subject, enron, methanol, meter, 988291, this...   \n",
              "1  [subject, hpl, nom, for, january, 9, 2001, see...   \n",
              "2  [subject, neon, retreat, ho, ho, ho, we, re, a...   \n",
              "3  [subject, photoshop, windows, office, cheap, m...   \n",
              "4  [subject, re, indian, springs, this, deal, is,...   \n",
              "\n",
              "                                        no_stopwords  \n",
              "0  [subject, enron, methanol, meter, 988291, foll...  \n",
              "1  [subject, hpl, nom, january, 9, 2001, see, att...  \n",
              "2  [subject, neon, retreat, ho, ho, ho, around, w...  \n",
              "3  [subject, photoshop, windows, office, cheap, m...  \n",
              "4  [subject, indian, springs, deal, book, teco, p...  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJBPI5g5k2Sh"
      },
      "source": [
        "##### Question 10 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0uZhx_nk2Si",
        "outputId": "d16eb9b3-6b63-4bf8-b326-d15d0ae9b672"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcP9yMQ-k2Si"
      },
      "outputs": [],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2nzQhvdk2Si"
      },
      "outputs": [],
      "source": [
        "def stem_words(words):\n",
        "    return [porter_stemmer.stem(word) for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy9y-Uh-k2Si"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xojW4Cnrk2Sj"
      },
      "outputs": [],
      "source": [
        "def lemmatize_words(words):\n",
        "    return [wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(word)) for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O68t8Lnk2Sv",
        "outputId": "97312a50-dc9e-40f4-ccf9-d90630050126"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_stem\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(stem_words)\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_lem\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_stopwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize_words\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36mlemmatize_words\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_words\u001b[39m(words):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [wordnet_lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, pos\u001b[38;5;241m=\u001b[39mget_wordnet_pos(word)) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n",
            "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_words\u001b[39m(words):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [wordnet_lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, pos\u001b[38;5;241m=\u001b[39m\u001b[43mget_wordnet_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n",
            "Cell \u001b[1;32mIn[36], line 2\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wordnet_pos\u001b[39m(word):\n\u001b[1;32m----> 2\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m      3\u001b[0m     tag_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV}\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag, wordnet\u001b[38;5;241m.\u001b[39mNOUN)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "df['text_stem'] = df['no_stopwords'].apply(stem_words)\n",
        "df['text_lem'] = df['no_stopwords'].apply(lemmatize_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nzDgOUOk2Sw"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNaH-ivIk2Sw"
      },
      "source": [
        "### Exercice 2 :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQReIbQRk2Sw"
      },
      "source": [
        "##### Question 1 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k6rH2z0_k2Sx"
      },
      "outputs": [],
      "source": [
        "para = \"\"\"A beutiful and kand-heartedd gerrl nammed Ella and her parints livve hapily in a latge\n",
        "hous withe a feiw sarvints, unttil her mather fals ille and laterr dies. Ella promise to folow her\n",
        "mother's last wish: to have corage and be kind. Yaers latter, Ella's farher mrries recently-widowd\n",
        "Ladi Tremaine, who has two unpleasent dauhters, Drisella and Anastasia. Ella's father leves on\n",
        "busines, and Lady Tremaine rveals her cruel and jalous natur, forcing Ella to give up her bedrom\n",
        "to the stepsisers and move into the atic. When Ella's father inexpectedly dies, Lady Tremaine\n",
        "dismissses the houshold staff to save mony, and forcez all of theire chores upon Ella. Seeing Ella's\n",
        "face covred in cinders after sleping by the firepplace, her step-family mokinglu dabs her\n",
        "\"Cinderella\".\"\"\"\n",
        "\n",
        "corrPara = \"\"\"A beautiful and kind-hearted girl named Ella and her parents live happily in a large\n",
        "house with a few servants, until her mother falls ill and later dies. Ella promises to follow her\n",
        "mother's last wish: to have courage and be kind. Years later, Ella's father marries recently-widowed\n",
        "Lady Tremaine, who has two unpleasant daughters, Drisella and Anastasia. Ella's father leaves on\n",
        "business, and Lady Tremaine reveals her cruel and jealous nature, forcing Ella to give up her\n",
        "bedroom to the stepsisters and move into the attic. When Ella's father unexpectedly dies, Lady\n",
        "Tremaine dismisses the household staff to save money, and forces all of their chores upon Ella.\n",
        "Seeing Ella's face covered in cinders after sleeping by the fireplace, her step-family mockingly\n",
        "dubs her \"Cinderella\".\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def corriger_para(texte):\n",
        "    spell = SpellChecker()\n",
        "    mots = texte.split()\n",
        "    mots_corr = []\n",
        "    for mot in mots:\n",
        "        if spell.correction(mot) is not None:\n",
        "            mots_corr.append(spell.correction(mot))\n",
        "        else:\n",
        "            mots_corr.append(mot)\n",
        "    texte_corr = ' '.join(mots_corr)\n",
        "    return texte_corr, mots, mots_corr\n"
      ],
      "metadata": {
        "id": "mR8mrshVpmMD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Wm1Vy2fDk2Sx"
      },
      "outputs": [],
      "source": [
        "para_corr, mots_avant, mots_apres = corriger_para(para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHzoy4snk2Sy",
        "outputId": "5c2b49db-ff11-4911-e0a7-0d8316803b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A beautiful and kand-heartedd girl named fella and her parents live happily in a late house withe a few sarvints, until her father fall ill and later dies fella promise to follow her mother's last wish to have courage and be kind years latter ell's father marries recently-widowd lady remained who has two unpleasant daughters Drisella and Anastasia. ell's father lives on business and Lady remain reveals her cruel and jealous nature forcing fella to give up her bedroom to the stepsisters and move into the stick When ell's father unexpectedly dies Lady remain dismisses the household staff to save many and force all of there chores upon fella Seeing ell's face covered in cinders after sleeping by the fireplace her step-family mockingly dabs her \"Cinderella\".\n"
          ]
        }
      ],
      "source": [
        "print(para_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YmeD8AFk2Sy"
      },
      "source": [
        "##### Question 2 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu2l7iC9k2Sy",
        "outputId": "99f79728-0305-483f-dd94-a71e436ac839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['beautiful', 'girl', 'named', 'fella', 'parents', 'live', 'happily', 'late', 'house', 'few', 'until', 'father', 'fall', 'ill', 'later', 'dies', 'fella', 'follow', 'wish', 'courage', 'kind', 'years', 'latter', \"ell's\", 'father', 'marries', 'lady', 'remained', 'unpleasant', 'daughters', \"ell's\", 'lives', 'business', 'remain', 'reveals', 'jealous', 'nature', 'fella', 'bedroom', 'stepsisters', 'stick', \"ell's\", 'unexpectedly', 'dies', 'remain', 'dismisses', 'household', 'many', 'force', 'there', 'fella', \"ell's\", 'covered', 'sleeping', 'fireplace', 'mockingly']\n"
          ]
        }
      ],
      "source": [
        "mots_corr = [mot_apres for mot_avant, mot_apres in zip(mots_avant, mots_apres) if mot_avant != mot_apres]\n",
        "print(mots_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqbSTpkPk2Sy"
      },
      "source": [
        "##### Question 3 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JH-peDrak2Sy"
      },
      "outputs": [],
      "source": [
        "def comparer_corrections(texte_original, texte_corrig, texte_reference):\n",
        "\n",
        "    mots_original = set(texte_original.split())\n",
        "    mots_corriges = set(texte_corrig.split())\n",
        "    mots_reference = set(texte_reference.split())\n",
        "\n",
        "    mots_corrects = mots_corriges.intersection(mots_reference)\n",
        "    mots_incorrects = mots_original.difference(mots_corrects)\n",
        "\n",
        "    return mots_corrects, mots_incorrects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gT8V8u0-k2Sz"
      },
      "outputs": [],
      "source": [
        "mots_corrects, mots_incorrects = comparer_corrections(para, para_corr, corrPara)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUp-XifMk2Sz"
      },
      "outputs": [],
      "source": [
        "mots_corrects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w54whdrk2Sz"
      },
      "outputs": [],
      "source": [
        "mots_incorrects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rMthvL-k2S0"
      },
      "source": [
        "##### Question 4 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2tYHbOpXk2S0"
      },
      "outputs": [],
      "source": [
        "nombre_total_mots = len(para.split())\n",
        "nombre_erreurs = len(mots_incorrects)\n",
        "taux_erreurs = (nombre_erreurs / nombre_total_mots) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTJ6QLwqk2S0",
        "outputId": "756aa8ed-df65-4b60-b759-95062c5e342e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44.44444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "taux_erreurs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu85XnBBk2S0"
      },
      "source": [
        "## Partie II :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1faQ7qWk2S1"
      },
      "source": [
        "### Exercice 1 :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 1 :"
      ],
      "metadata": {
        "id": "96vm_pFHryU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "language = wordnet.synsets('language')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDetgJNYrxVv",
        "outputId": "5ea33496-93a1-4f05-c245-094447b10f97"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for synset in language:\n",
        "    print(\"Definition :\", synset.definition())\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9YgS_kVtfiD",
        "outputId": "37f61308-226e-4d39-cfac-619a01c78831"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition : a systematic means of communicating by the use of sounds or conventional symbols\n",
            "--------------------------------------------------\n",
            "Definition : (language) communication by word of mouth\n",
            "--------------------------------------------------\n",
            "Definition : the text of a popular song or musical-comedy number\n",
            "--------------------------------------------------\n",
            "Definition : the cognitive processes involved in producing and understanding linguistic communication\n",
            "--------------------------------------------------\n",
            "Definition : the mental faculty or power of vocal communication\n",
            "--------------------------------------------------\n",
            "Definition : a system of words used to name things in a particular discipline\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 2 :"
      ],
      "metadata": {
        "id": "1b1xo-cZtngW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentences = []\n",
        "for synset in language:\n",
        "    for lemma in synset.lemmas():\n",
        "        examples = lemma.synset().examples()\n",
        "        example_sentences.extend(examples)\n",
        "\n",
        "if example_sentences:\n",
        "    print(\"Example Sentence: \", example_sentences[0])\n",
        "else:\n",
        "    print(\"No example sentences found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4deEJKa_tibE",
        "outputId": "6b93d870-6671-4321-d6c0-6ac3631e5feb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Sentence:  he taught foreign languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 3 :"
      ],
      "metadata": {
        "id": "irsa-v6suEeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU9oLg-Gvmvt",
        "outputId": "3dd439d1-a313-4292-dbaf-5db889e0fbd9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for synset in language:\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Definition (en anglais):\", synset.definition())\n",
        "    for lemma in synset.lemmas():\n",
        "        print(\"Lemme (anglais):\", lemma.name())\n",
        "        for lang_code in ['eng', 'ita', 'fra', 'arb']:\n",
        "            translation = lemma.synset().lemma_names(lang_code)\n",
        "            if translation:\n",
        "                print(f\"Lemme ({lang_code}):\", translation[0])\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBRQ0MuxuGYI",
        "outputId": "668bbe3b-0c2f-4b01-d14e-7b348d3d32e8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Definition (en anglais): a systematic means of communicating by the use of sounds or conventional symbols\n",
            "Lemme (anglais): language\n",
            "Lemme (eng): language\n",
            "Lemme (ita): idioma\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): لُغة\n",
            "----------\n",
            "Lemme (anglais): linguistic_communication\n",
            "Lemme (eng): language\n",
            "Lemme (ita): idioma\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): لُغة\n",
            "----------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): (language) communication by word of mouth\n",
            "Lemme (anglais): speech\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): speech_communication\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): spoken_communication\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): spoken_language\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): language\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): voice_communication\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): oral_communication\n",
            "Lemme (eng): speech\n",
            "Lemme (ita): lingua\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): the text of a popular song or musical-comedy number\n",
            "Lemme (anglais): lyric\n",
            "Lemme (eng): lyric\n",
            "Lemme (ita): parole\n",
            "Lemme (fra): langage\n",
            "----------\n",
            "Lemme (anglais): words\n",
            "Lemme (eng): lyric\n",
            "Lemme (ita): parole\n",
            "Lemme (fra): langage\n",
            "----------\n",
            "Lemme (anglais): language\n",
            "Lemme (eng): lyric\n",
            "Lemme (ita): parole\n",
            "Lemme (fra): langage\n",
            "----------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): the cognitive processes involved in producing and understanding linguistic communication\n",
            "Lemme (anglais): linguistic_process\n",
            "Lemme (eng): linguistic_process\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): عملِيّة_لُغوِيّة\n",
            "----------\n",
            "Lemme (anglais): language\n",
            "Lemme (eng): linguistic_process\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): عملِيّة_لُغوِيّة\n",
            "----------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): the mental faculty or power of vocal communication\n",
            "Lemme (anglais): language\n",
            "Lemme (eng): language\n",
            "Lemme (ita): favella\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "Lemme (anglais): speech\n",
            "Lemme (eng): language\n",
            "Lemme (ita): favella\n",
            "Lemme (fra): langage\n",
            "Lemme (arb): كلام\n",
            "----------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): a system of words used to name things in a particular discipline\n",
            "Lemme (anglais): terminology\n",
            "Lemme (eng): terminology\n",
            "Lemme (ita): nomenclatura\n",
            "Lemme (fra): Nomenclature\n",
            "----------\n",
            "Lemme (anglais): nomenclature\n",
            "Lemme (eng): terminology\n",
            "Lemme (ita): nomenclatura\n",
            "Lemme (fra): Nomenclature\n",
            "----------\n",
            "Lemme (anglais): language\n",
            "Lemme (eng): terminology\n",
            "Lemme (ita): nomenclatura\n",
            "Lemme (fra): Nomenclature\n",
            "----------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 4 :"
      ],
      "metadata": {
        "id": "cyehfMM5wUVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for synset in language:\n",
        "#     print(\"-\" * 50)\n",
        "#     print(\"Definition (en anglais):\", synset.definition())\n",
        "#     for lemma in synset.lemmas():\n",
        "#         synonyms = [synonym.name() for synonym in lemma.synset().lemmas()]\n",
        "#         if synonyms:\n",
        "#             print(f\"Synonymes (anglais): {', '.join(synonyms)}\")\n",
        "#     print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMqFWDlAwaJf",
        "outputId": "b69ee50a-3eb0-4f5a-8815-f643c79c0f30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Definition (en anglais): a systematic means of communicating by the use of sounds or conventional symbols\n",
            "Synonymes (anglais): language, linguistic_communication\n",
            "Synonymes (anglais): language, linguistic_communication\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): (language) communication by word of mouth\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "Synonymes (anglais): speech, speech_communication, spoken_communication, spoken_language, language, voice_communication, oral_communication\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): the text of a popular song or musical-comedy number\n",
            "Synonymes (anglais): lyric, words, language\n",
            "Synonymes (anglais): lyric, words, language\n",
            "Synonymes (anglais): lyric, words, language\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): the cognitive processes involved in producing and understanding linguistic communication\n",
            "Synonymes (anglais): linguistic_process, language\n",
            "Synonymes (anglais): linguistic_process, language\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): the mental faculty or power of vocal communication\n",
            "Synonymes (anglais): language, speech\n",
            "Synonymes (anglais): language, speech\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Definition (en anglais): a system of words used to name things in a particular discipline\n",
            "Synonymes (anglais): terminology, nomenclature, language\n",
            "Synonymes (anglais): terminology, nomenclature, language\n",
            "Synonymes (anglais): terminology, nomenclature, language\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execice 2 :"
      ],
      "metadata": {
        "id": "w_9dMmom0j9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 1 :"
      ],
      "metadata": {
        "id": "AY9EHh760oL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('state_union')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ikAwKDgxgcZ",
        "outputId": "c3951433-453f-4250-cef8-4d1a0e0859f3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 2 :"
      ],
      "metadata": {
        "id": "Zwo3NIxa1SI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sotu_words = state_union.words()\n",
        "filtered_words = [word for word in sotu_words if word.isalpha()]"
      ],
      "metadata": {
        "id": "myCnW_Vr1F6Z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yceTbwy52JFt",
        "outputId": "c22d1ee7-54a0-4ec1-9d27-dc066c8fd337"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PRESIDENT',\n",
              " 'HARRY',\n",
              " 'S',\n",
              " 'TRUMAN',\n",
              " 'S',\n",
              " 'ADDRESS',\n",
              " 'BEFORE',\n",
              " 'A',\n",
              " 'JOINT',\n",
              " 'SESSION']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 3 :"
      ],
      "metadata": {
        "id": "w8rjmotD3TJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words_no_stopwords = [word for word in filtered_words if word not in stop_words]"
      ],
      "metadata": {
        "id": "mQuBEVXF3Kw_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words_no_stopwords[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4D762GQ3fkg",
        "outputId": "e1ed5b85-5789-4a90-ad33-3bafadcd7d55"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PRESIDENT',\n",
              " 'HARRY',\n",
              " 'S',\n",
              " 'TRUMAN',\n",
              " 'S',\n",
              " 'ADDRESS',\n",
              " 'BEFORE',\n",
              " 'A',\n",
              " 'JOINT',\n",
              " 'SESSION']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 4 :"
      ],
      "metadata": {
        "id": "jM0EA9p03grn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = nltk.FreqDist(filtered_words_no_stopwords)\n",
        "freq_dist.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9aHvMJ_3zsa",
        "outputId": "77182ae7-4b05-4d48-feb8-ac4a9ac8c89b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 3394),\n",
              " ('We', 2063),\n",
              " ('must', 1568),\n",
              " ('The', 1520),\n",
              " ('people', 1291),\n",
              " ('world', 1128),\n",
              " ('year', 1097),\n",
              " ('America', 1076),\n",
              " ('And', 1057),\n",
              " ('us', 1049)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mot_test = \"america\"\n",
        "\n",
        "print(f\"\\nFréquence de '{mot_test.lower()}': {freq_dist[mot_test.lower()]}\")\n",
        "print(f\"Fréquence de '{mot_test.capitalize()}': {freq_dist[mot_test.capitalize()]}\")\n",
        "print(f\"Fréquence de '{mot_test.upper()}': {freq_dist[mot_test.upper()]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBGDTYL86MNx",
        "outputId": "d2553be0-015f-44f6-a381-f2fc94758e96"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fréquence de 'america': 0\n",
            "Fréquence de 'America': 1076\n",
            "Fréquence de 'AMERICA': 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 5 :"
      ],
      "metadata": {
        "id": "ZppemUlA6xzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words = [word.lower() for word in sotu_words if word.isalpha() and word.lower() not in set(stopwords.words('english'))]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "normalized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "normalized_freq_dist = nltk.FreqDist(normalized_words)"
      ],
      "metadata": {
        "id": "WM0P_Nxb6OAl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_freq_dist.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UblDxr-V7soL",
        "outputId": "18164d31-bdd3-49a6-8172-fb37b194fd84"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('year', 1933),\n",
              " ('must', 1569),\n",
              " ('people', 1395),\n",
              " ('american', 1348),\n",
              " ('nation', 1261),\n",
              " ('world', 1213),\n",
              " ('new', 1112),\n",
              " ('america', 1088),\n",
              " ('congress', 1084),\n",
              " ('u', 1082)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 6 :"
      ],
      "metadata": {
        "id": "ul2N7enQ8suU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [word.lower() for word in sotu_words if word.isalpha()]\n",
        "text = Text(all_words)\n",
        "mot_test = \"america\"\n",
        "concordances = text.concordance(mot_test, width=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40zdUnlC7xp8",
        "outputId": "fd771ff0-f4e5-4fb2-b209-29cbb6d2fb5a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 25 of 1079 matches:\n",
            "t is what he would want us to do that is what america will do so much blood has already been shed f\n",
            " victory today the entire world is looking to america for enlightened leadership to peace and progr\n",
            " be certain beyond any shadow of a doubt that america will continue the fight for freedom until no \n",
            " heavy price to make complete victory certain america will never become a party to any plan for par\n",
            "l be found only in law and in justice here in america we have labored long and hard to achieve a so\n",
            "e let me assure the forward looking people of america that there w ill be no relaxation in our effo\n",
            "ntain our american way of life at this moment america along with her brave allies is paying again a\n",
            "repaid they have earned our undying gratitude america will never forget their sacrifices because of\n",
            "g national differences must and will be found america must assist suffering humanity back along the\n",
            "ch franklin delano roosevelt always had today america has become one of the most powerful forces fo\n",
            " live up to our glorious heritage in that way america may well lead the world to peace and prosperi\n",
            "loyers f agricultural programs the farmers of america generally are entering the crop year of in be\n",
            "process of economic development the rivers of america offer a great opportunity to our generation i\n",
            "the development of the great river systems of america there is the major opportunity of our generat\n",
            "hould bar any citizen of the united states of america from an education or from good health or from\n",
            "f this leadership will involve application of america s influence in world affairs with such fortit\n",
            "fficiency economy and integrity the safety of america and the trust of the people alike demand that\n",
            "future the best natural resources program for america will not result from exclusive dependence on \n",
            "to play a significant role in the planning of america s economic future to that end i am authorizin\n",
            "ese is our great and growing body of veterans america has traditionally been generous in caring for\n",
            " that objective is the building of a stronger america a nation whose every citizen has good reason \n",
            "e is secure that is what i mean by a stronger america toward this objective a real momentum has bee\n",
            "n the forward road to a better and a stronger america all my recommendations today are in furtheran\n",
            "y council this new program will make and keep america strong in an age of peril nothing should bar \n",
            "ery farmer knows he cannot prosper unless all america prospers as we seek to promote increases in o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question 7 :"
      ],
      "metadata": {
        "id": "JiwWnRUm9JqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder, QuadgramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures, QuadgramAssocMeasures"
      ],
      "metadata": {
        "id": "XuXNzR8mAmWN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_collocations = BigramCollocationFinder.from_words(text)\n",
        "bigram_collocations = bigram_collocations.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
        "\n",
        "trigram_collocations = TrigramCollocationFinder.from_words(text)\n",
        "trigram_collocations = trigram_collocations.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
        "\n",
        "quadgram_collocations = QuadgramCollocationFinder.from_words(text)\n",
        "quadgram_collocations = quadgram_collocations.nbest(QuadgramAssocMeasures.likelihood_ratio, 10)"
      ],
      "metadata": {
        "id": "1cOTlWZh9LxA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bigrammes les plus courants:\")\n",
        "print(bigram_collocations)\n",
        "\n",
        "print(\"\\nTrigrammes les plus courants:\")\n",
        "print(trigram_collocations)\n",
        "\n",
        "print(\"\\nQuadgrammes les plus courants:\")\n",
        "print(quadgram_collocations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vt0dwA3Ahrh",
        "outputId": "a36311b7-2c2e-4cd8-b0c5-d068ca20875c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrammes les plus courants:\n",
            "[('we', 'must'), ('united', 'states'), ('in', 'the'), ('of', 'the'), ('we', 'have'), ('will', 'be'), ('more', 'than'), ('the', 'the'), ('it', 'is'), ('the', 'congress')]\n",
            "\n",
            "Trigrammes les plus courants:\n",
            "[('the', 'congress', 'of'), ('the', 'world', 'of'), ('the', 'state', 'of'), ('the', 'next', 'of'), ('the', 'members', 'of'), ('the', 'part', 'of'), ('the', 'most', 'of'), ('the', 'first', 'of'), ('the', 'millions', 'of'), ('the', 'union', 'of')]\n",
            "\n",
            "Quadgrammes les plus courants:\n",
            "[('of', 'the', 'congress', 'of'), ('of', 'the', 'world', 'of'), ('in', 'the', 'congress', 'of'), ('the', 'congress', 'of', 'the'), ('in', 'the', 'world', 'of'), ('the', 'world', 'of', 'the'), ('of', 'the', 'state', 'of'), ('the', 'state', 'of', 'the'), ('of', 'the', 'members', 'of'), ('in', 'the', 'state', 'of')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygKhdiFHBHrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sobMg3f_k2SN",
        "IJbkNnJ4k2SQ",
        "2hmYcnYUk2SR",
        "IA5hUA_yk2SW",
        "pNmG0UmMk2SY",
        "GMQJ7cQIk2SZ",
        "49or_R7ik2Sb",
        "ym9j3_Ekk2Sc",
        "rutWsyXZk2Sd",
        "PPlsj2sqk2Sd",
        "AwujevEDk2Se",
        "jJBPI5g5k2Sh",
        "LNaH-ivIk2Sw",
        "oQReIbQRk2Sw",
        "_YmeD8AFk2Sy",
        "iqbSTpkPk2Sy",
        "5rMthvL-k2S0",
        "B1faQ7qWk2S1",
        "96vm_pFHryU0",
        "1b1xo-cZtngW",
        "irsa-v6suEeg",
        "cyehfMM5wUVs"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}